[cerebrium.deployment]
name = "cooking-llama-inference"
python_version = "3.11"
docker_base_image_url = "debian:bookworm-slim"
disable_auth = false
include = ["main.py", "cerebrium.toml"]

[cerebrium.dependencies.paths]
pip = "requirements.txt"

[cerebrium.hardware]
cpu = 2
memory = 8.0
compute = "TURING_T4"
gpu_count = 1

[cerebrium.scaling]
min_replicas = 0
max_replicas = 1
cooldown = 100
replica_concurrency = 1
response_grace_period = 900
scaling_metric = "concurrency_utilization"
scaling_target = 100
scaling_buffer = 0
roll_out_duration_seconds = 0